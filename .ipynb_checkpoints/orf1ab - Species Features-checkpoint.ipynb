{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the features of different species.\n",
    "- we will analyse all the species that has atleast 20 counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from eli5 import show_weights, show_prediction\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from eli5 import show_weights\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import display, clear_output\n",
    "import eli5\n",
    "import joblib\n",
    "\n",
    "#custom\n",
    "from py.orf1ab_dash_board import DataProcessing, get_dashboard\n",
    "from py.ml_metrics import evaluate_model, multiclass_logloss\n",
    "from py.plotting import plot_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the  following dashboard to get some context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dashboard(df_columns=[\"ALL\", \"Species\", 'Geo_Location', 'Host', 'Isolation_Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(orf1):  \n",
    "    # read for data folder and out put \n",
    "    df = orf1.get_amino_df()\n",
    "    print(f\"shape WITH duplicates: {df.shape}\")\n",
    "\n",
    "    # remove duplicates\n",
    "    df.drop_duplicates(subset='Accession', keep=False, inplace=True)\n",
    "    print(f\"shape WITHOUT duplicates: {df.shape}\")\n",
    "    df['Collection_Date'] = pd.to_datetime(df['Collection_Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    df['Release_Date'] = pd.to_datetime(df['Release_Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    df['Length'] = df['Length'].apply(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and select only those species that are atleast 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_column(df, column_name, min_count):\n",
    "    '''\n",
    "    df: dataframe\n",
    "    column_name: column to filter\n",
    "    min_count: minimum count required to be included\n",
    "    '''\n",
    "    counts = Counter(df[column_name])\n",
    "    filtered = [key for key in counts if counts[key] >= min_count]\n",
    "    print(filtered)\n",
    "    df = df[df[column_name].isin(filtered)]\n",
    "    return df[df[column_name].notna()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_classes(df, column_name):\n",
    "    #labels\n",
    "    lbl_enc = LabelEncoder()\n",
    "    y = lbl_enc.fit_transform(df[column_name].values)\n",
    "\n",
    "    # map labels to numercial values\n",
    "    #map labels to numerical value\n",
    "    labels = list(lbl_enc.inverse_transform(y))\n",
    "    return dict(zip(y, labels)), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df, column_name, kmer, results_path):\n",
    "    \n",
    "    class_dict, y = map_classes(df, column_name)\n",
    "    #train test split\n",
    "    xtrain, xvalid, ytrain, yvalid = train_test_split(df['seq'].values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "    #vectorize\n",
    "    ctv = CountVectorizer(analyzer='char', ngram_range=(kmer, kmer), lowercase=False) # kmer: k-mer length\n",
    "    # embed\n",
    "    ctv.fit(list(xtrain)+list(xvalid))\n",
    "    xtrain_ctv = ctv.transform(xtrain)\n",
    "    xvalid_ctv = ctv.transform(xvalid)\n",
    "    \n",
    "    # Fitting a simple Logistic Regression on Counts\n",
    "    clf = LogisticRegression(C=1.0, max_iter=4000)\n",
    "    clf.fit(xtrain_ctv, ytrain)\n",
    "    predictions = clf.predict(xvalid_ctv)\n",
    "    \n",
    "    #make report\n",
    "    report = classification_report(yvalid, predictions, \n",
    "                                   target_names=class_dict.values(), output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    #extract features\n",
    "    feature_imp = eli5.formatters.as_dataframe.explain_weights_df(clf, \n",
    "    feature_names=ctv.get_feature_names())\n",
    "    feature_imp = feature_imp.replace({\"target\": class_dict})\n",
    "    \n",
    "    model_root = os.path.join(results_path, 'models')\n",
    "    data_root = os.path.join(results_path, column_name)\n",
    "    if not os.path.exists(data_root):\n",
    "        os.mkdir(data_root)\n",
    "        \n",
    "    \n",
    "        \n",
    "    # save results\n",
    "    #model \n",
    "    joblib.dump(clf, os.path.join(model_root, f\"orf1_{column_name}_{k}_mer_lr_model.pkl\"), compress=9)\n",
    "    # predictions\n",
    "    pred_label = [class_dict[i] for i in predictions]\n",
    "    pd.DataFrame({column_name:pred_label, \"pred\":predictions}).to_csv(\n",
    "        os.path.join(data_root, f\"orf1_{column_name}_{k}_mer_lr_pred.csv\"), index=False, header=True)\n",
    "\n",
    "        \n",
    "    #classification report\n",
    "    report.to_csv(os.path.join(data_root, f\"orf1_{column_name}_{k}_mer_lr_metrics.csv\"), \n",
    "                  index=True, header=True)\n",
    "    \n",
    "    # feature importance\n",
    "    feature_imp.to_csv(os.path.join(data_root, f\"orf1_{column_name}_{k}_mer_lr_feature.csv\"),\n",
    "                       index=False, header=True)\n",
    "    clear_output()\n",
    "    display(report)\n",
    "    display(show_weights(clf, vec=ctv, top=25, feature_filter=lambda x: x != '<BIAS>', \n",
    "                         target_names=class_dict))\n",
    "    print(f\"---> Analysis of {k}-mer done! Results in {data_root}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_ROOT = os.path.dirname(os.path.realpath('__file__'))\n",
    "DATA_PATH = Path(Path(BOOK_ROOT).resolve().parent, \"data\")\n",
    "TOOLS_PATH = Path(Path(BOOK_ROOT).resolve().parent, \"tools\")\n",
    "PLOTS_PATH = Path(Path(BOOK_ROOT).resolve().parent, \"plots\")\n",
    "RESULTS_PATH = Path(Path(BOOK_ROOT).resolve().parent, \"results\")\n",
    "\n",
    "\n",
    "orf1 = DataProcessing('coronavirus_orf1ab.fasta', 'coronavirus_orf1ab_meta.csv')\n",
    "data = get_data(orf1)\n",
    "for column_name in ['Species']:\n",
    "    df = filter_column(data, column_name, 20)\n",
    "#     print(df.head())\n",
    "    for k in range(10, 50):\n",
    "        model_predict(df, column_name, k, RESULTS_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
